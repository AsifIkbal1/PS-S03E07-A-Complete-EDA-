{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!wget http://bit.ly/3ZLyF82 -O CSS.css -q\n    \nfrom IPython.core.display import HTML\nwith open('./CSS.css', 'r') as file:\n    custom_css = file.read()\n\nHTML(custom_css)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-14T03:24:23.817812Z","iopub.execute_input":"2023-02-14T03:24:23.818253Z","iopub.status.idle":"2023-02-14T03:24:25.225865Z","shell.execute_reply.started":"2023-02-14T03:24:23.818217Z","shell.execute_reply":"2023-02-14T03:24:25.224304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PS S03E07: A Complete EDA ⭐️\n\nThis EDA gives useful insights when designig a machine learning pipeline for this episode competition\n\n**Versions**\n- v1: Inital EDA\n- v2: Some feature egineering on dates\n\n# Table of Content\n\n1. [The Data](#The-Data)\n1. [The Label](#The-Label)\n1. [EDA](#EDA)\n    1. [Data Size](#Data-Size)\n1. [Distributions](#Distributions)\n    1. [Numerical + Ordinal Features](#Numerical-+-Ordinal-Features)\n    1. [Categorical Columns](#Categorical-Columns)\n1. [Date Feature Engineering](#Date-Feature-Engineering)\n1. [Date Features](#Date-Features)\n    1. [Wrong dates](#Wrong-dates)\n    1. [Date-based Features](#Date-based-Features)\n1. [Missing Values](#Missing-Values)\n1. [Duplicates](#Duplicates)\n1. [Correlations](#Correlations)\n1. [Basic Baseline](#Basic-Baseline)\n1. [Submission](#Submission)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from time import time\nfrom datetime import timedelta\nfrom colorama import Fore, Style\n\nimport math\nimport matplotlib\nimport matplotlib as mpl\nimport matplotlib.cm as cmap\nimport matplotlib.colors as mpl_colors\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport lightgbm as lgbm\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy\nimport warnings\nfrom cycler import cycler\n\npalette = ['#3c3744', '#048BA8', '#EE6352', '#E1BB80', '#78BC61']\ngrey_palette = [\n    '#8e8e93', '#636366', '#48484a', '#3a3a3c', '#2c2c2e', '#1c1c27'\n]\n\nbg_color = '#F6F5F5'\nwhite_color = '#d1d1d6'\n\ncustom_params = {\n    \"axes.spines.right\": False,\n    \"axes.spines.top\": False,\n    \"axes.spines.left\": False,\n    'grid.alpha':0.2,\n    'figure.figsize': (16, 6),\n    'axes.titlesize': 'large',\n    'axes.labelsize': 'large',\n    'ytick.labelsize': 'medium',\n    'xtick.labelsize': 'medium',\n    'legend.fontsize': 'large',\n    'lines.linewidth': 1,\n    'axes.prop_cycle': cycler('color',palette),\n    'figure.facecolor': bg_color,\n    'figure.edgecolor': bg_color,\n    'axes.facecolor': bg_color,\n    'text.color':grey_palette[1],\n    'axes.labelcolor':grey_palette[1],\n    'axes.edgecolor':grey_palette[1],\n    'xtick.color':grey_palette[1],\n    'ytick.color':grey_palette[1],\n    'figure.dpi':150,\n}\n\nsns.set_theme(\n    style='whitegrid',\n    palette=sns.color_palette(palette),\n    rc=custom_params\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-14T03:24:25.229347Z","iopub.execute_input":"2023-02-14T03:24:25.230234Z","iopub.status.idle":"2023-02-14T03:24:25.248592Z","shell.execute_reply.started":"2023-02-14T03:24:25.230172Z","shell.execute_reply":"2023-02-14T03:24:25.247284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Data\n\nThe dataset was generated using [Reservation Cancellation Prediction](https://www.kaggle.com/datasets/gauravduttakiit/reservation-cancellation-prediction). The task is to predict the right `booking_status` which is an binary column indicating whether the reservation was cancelled or not.\n\nSome key aspects are:\n\n1 Competition dataset generated from Reservation Cancellation Prediction dataset\n2 Deep learning model used to generate both train and test dataset\n3 Feature distributions are similar but not exactly the same as the original dataset\n4 Original dataset can be used as part of competition to explore differences and improve model performance\n\n---\nColumns description from original [Reservation Cancellation Prediction](https://www.kaggle.com/datasets/gauravduttakiit/reservation-cancellation-prediction)\n\nThe file contains the different attributes of customers' reservation details. The detailed data dictionary is given below\n* id: unique identifier of each booking\n* no_of_adults: Number of adults\n* no_of_children: Number of Children\n* no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n* no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n* type_of_meal_plan: Type of meal plan booked by the customer:\n* required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)\n* room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.\n* lead_time: Number of days between the date of booking and the arrival date\n* arrival_year: Year of arrival date\n* arrival_month: Month of arrival date\n* arrival_date: Date of the month\n* market_segment_type: Market segment designation.\n* repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)\n* no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking\n* no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking\n* avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)\n* no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)\n\nOutput variable:\n- booking_status (0 or 1)","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s3e7/train.csv', index_col=0)\ntest_df = pd.read_csv('/kaggle/input/playground-series-s3e7/test.csv', index_col=0)\n\n# load original dataset\noriginal_train_df = pd.read_csv('/kaggle/input/reservation-cancellation-prediction/train__dataset.csv')\noriginal_test_df = pd.read_csv('/kaggle/input/reservation-cancellation-prediction/test___dataset.csv')\n\n\noriginal_train_df.index.name = 'id'\noriginal_test_df.index.name = 'id'","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:24:25.250578Z","iopub.execute_input":"2023-02-14T03:24:25.251572Z","iopub.status.idle":"2023-02-14T03:24:25.442107Z","shell.execute_reply.started":"2023-02-14T03:24:25.251529Z","shell.execute_reply":"2023-02-14T03:24:25.440777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Label\n\n\nThe label target seems to follow the same distribution, this is good in the sense that augmenting the dataset will have great power as we have seen in prevous competitions. Apart from having the same distribution, the label is no imbalaced, so this competition won't involve any oversampling, undersampling nor smote.\n\n**Insights**\n- StratifiedKFold is recommended as the initial cross validations strategy.\n- Further is explained that dates have very little influence on cross validation as train and test shares the same dates.","metadata":{}},{"cell_type":"code","source":"print('Train')\ndisplay(train_df.booking_status.value_counts(True))\n\nprint('\\nOriginal')\ndisplay(original_train_df.booking_status.value_counts(True))","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:24:25.44497Z","iopub.execute_input":"2023-02-14T03:24:25.445575Z","iopub.status.idle":"2023-02-14T03:24:25.464022Z","shell.execute_reply.started":"2023-02-14T03:24:25.445505Z","shell.execute_reply":"2023-02-14T03:24:25.463034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\n## Data Size\n\n**Insights**:\n- This a fairly light dataset, but compared to episode 5, we have enough data to validate if CV results correlates with LB. In this competition if test dataset has similar distributions than train, CV is a proxy of LB.\n- Original dataset has less records that synthetic dataset, beware of duplicates in both datasets.","metadata":{}},{"cell_type":"code","source":"print('Train shape:            ', train_df.shape)\nprint('Test shape:             ', test_df.shape)\nprint('Original Train shape:   ', original_train_df.shape)\nprint('Original Test shape:    ', original_test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:24:25.466054Z","iopub.execute_input":"2023-02-14T03:24:25.466419Z","iopub.status.idle":"2023-02-14T03:24:25.476664Z","shell.execute_reply.started":"2023-02-14T03:24:25.466386Z","shell.execute_reply":"2023-02-14T03:24:25.475358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distributions\n\nIt's important to check the distribution of data, as it gives us information about any anomalies in the variables and whether preprocessing is necessary. The next section will use basic plots to show relationships between the features, the target, and any differences between the synthetic and original datasets.\n\n## Numerical + Ordinal Features\n\n**Insights**:\n\n- Most of the features are counts, so they can be threated as ordinal features that is why we can use lineplots to visualize the relationship.\n- At first glance there are no discernable difference between train-test and original dataset distributions, supporting the idea of including them as part of our training exampels.\n- The relationship between features and target is different in the synthetic and original datasets, with the feature `no_of_week_nights` being the most concerning difference for outliers. The model that include this feature and original dataset should handle outliers or original dataset will polute the training dataset.\n- `no_of_previous_bookings_not_canceled` is mostly zero everywhere either because most people that repeat this hotel won't cancel the booking or beacuse this is the first time this group of people goes to the hotel. People exhibits a repetetive pattern the more cancellations they do.\n- If the number of special request increases, the less likely are the people to cancel their booking.","metadata":{}},{"cell_type":"code","source":"# Some helper Function\ndef plot_dots_ordinal(feature, ax):\n    dots = total_df.groupby(['set', feature])['booking_status'].mean().reset_index(level=1)\n    dots.sort_values(feature, inplace=True)\n    train_containers = ax.containers[0]\n    original_containers = ax.containers[2]\n    \n    containers = [train_containers, original_containers]\n    sets = ['train', 'original_train']\n    colors = ['#78BC61', '#FF7F50']\n    counter = 0\n    \n    for set_, container in zip(sets, containers):\n        dots_subset = dots.loc[set_]\n        \n        x_s = [bar.get_x() + bar.get_width()/2 for bar in container]\n        y_s = dots_subset.booking_status\n        x_s = x_s[:y_s.shape[0]]\n        \n        ax.plot(x_s, y_s, marker='.', alpha=0.8, \n                linestyle=line_style, markersize=10,\n                color=colors[counter]\n        )\n        \n        counter += 1\n\ndef plot_ordinals(feature, ax):\n    percentage = total_df.groupby('set')[feature].value_counts(True)\n    percentage = percentage.rename('%').reset_index()\n    sns.barplot(data=percentage, x=feature, y='%',\n                hue='set',ax=ax, hue_order=labels)\n    \n    if percentage.shape[0] > 100:\n        ticks = ax.get_xticks()\n        text = ax.get_xticklabels()\n        \n        step = len(ticks)//8\n        ax.set_xticks(ticks[::step], text[::step])\n        \n    plot_dots_ordinal(feature, ax)\n        \ndef plot_continous(feature, ax):\n    sns.histplot(data=total_df, x=feature,\n                hue='set',ax=ax, hue_order=labels,\n                common_norm=False, **histplot_hyperparams)\n    \n    ax_2 = ax.twinx()\n    ax_2 = plot_dot_continous(\n        total_df.query('set==\"train\"'),\n        feature, 'booking_status', ax_2,\n        color='#78BC61'\n    )\n    \n    ax_2 = plot_dot_continous(\n        total_df, feature,\n        'booking_status', ax_2,\n        color='#FF7F50'\n    )\n#     ax.legend(handles, legend_labels)\n        \n    \ndef plot_dot_continous(\n    df, column, target, ax,\n    show_yticks=False, color='green'\n):\n\n    bins = pd.cut(df[column], bins=n_bins)\n    bins = pd.IntervalIndex(bins)\n    bins = (bins.left + bins.right) / 2\n    target = df[target]\n    target = target.groupby(bins).mean()\n    target.plot(\n        ax=ax, linestyle='',\n        marker='.', color=color,\n        label=f'Mean {target.name}'\n    )\n    ax.grid(visible=False)\n    \n    if not show_yticks:\n        ax.get_yaxis().set_ticks([])\n        \n    return ax\n\n\ntotal_df = pd.concat([\n    train_df.assign(set='train'),\n    test_df.assign(set='test'),\n    original_train_df.assign(set='original_train'),\n    original_test_df.assign(set='original_test')\n], ignore_index=True)\n\ntotal_df.reset_index(drop=True, inplace=True)\nlabels = ['train', 'test', 'original_train', 'original_test']\n\nordinal_features = [\n    'no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights',\n    'no_of_special_requests', 'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled'\n]\n\nnumeric_features = [\n    'lead_time', 'avg_price_per_room'\n]\n\nn_bins = 50\nhistplot_hyperparams = {\n    'kde':True,\n    'alpha':0.4,\n    'stat':'percent',\n    'bins':n_bins\n}\nline_style='--'\n\ncolumns =  ordinal_features + numeric_features\nn_cols = 3\nn_rows = math.ceil(len(columns)/n_cols)\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(16, n_rows*5))\nax = ax.flatten()\n\nfor i, column in enumerate(columns):\n    plot_axes = [ax[i]]\n    \n    if column in ordinal_features:\n        plot_ordinals(column, ax[i])\n    else:\n        plot_continous(column, ax[i])\n\n    # titles\n    ax[i].set_title(f'{column} Distribution');\n    ax[i].set_xlabel(None)\n    \nfor i in range(i+1, len(ax)):\n    ax[i].axis('off')\n\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-14T03:28:16.579266Z","iopub.execute_input":"2023-02-14T03:28:16.579671Z","iopub.status.idle":"2023-02-14T03:28:26.598786Z","shell.execute_reply.started":"2023-02-14T03:28:16.57964Z","shell.execute_reply":"2023-02-14T03:28:26.597712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical Columns\n\n**Insights**:\n\n- Categorical features exhibits the same discrepancy between synthetic and original dataset when analyzing the relantionship with the target column.\n- `type_of_meal_plan` should not be included in a linear model with augmented dataset becasuse the distributions are quite different and the rank of categories by target-mean is different between synthetic and original.\n- `room_type_reserved` has clear relationship with the target.","metadata":{}},{"cell_type":"code","source":"categorical_columns = [\n    'market_segment_type', 'repeated_guest', 'required_car_parking_space',\n    'room_type_reserved', 'type_of_meal_plan'\n]\nline_style=''\n\ncolumns = categorical_columns\nn_cols = 3\nn_rows = math.ceil(len(columns)/n_cols)\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(16, n_rows*5))\nax = ax.flatten()\n\nfor i, column in enumerate(columns):\n    plot_axes = [ax[i]]\n    plot_ordinals(column, ax[i])\n\n    # titles\n    ax[i].set_title(f'{column} Distribution');\n    ax[i].set_xlabel(None)\n    \nfor i in range(i+1, len(ax)):\n    ax[i].axis('off')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-14T03:28:26.600872Z","iopub.execute_input":"2023-02-14T03:28:26.604634Z","iopub.status.idle":"2023-02-14T03:28:28.388886Z","shell.execute_reply.started":"2023-02-14T03:28:26.604587Z","shell.execute_reply":"2023-02-14T03:28:28.387515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Date Feature Engineering","metadata":{}},{"cell_type":"code","source":"def process_time(df):\n    temp = df.rename(columns={\n        'arrival_year': 'year',\n        'arrival_month': 'month',\n        'arrival_date': 'day'\n    })\n\n    df['date'] = pd.to_datetime(temp[['year', 'month', 'day']], errors='coerce')\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week.astype(float)\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    \n    df.drop(columns='date', inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:28.390342Z","iopub.execute_input":"2023-02-14T03:28:28.390712Z","iopub.status.idle":"2023-02-14T03:28:28.40091Z","shell.execute_reply.started":"2023-02-14T03:28:28.390679Z","shell.execute_reply":"2023-02-14T03:28:28.399246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Date Features\n\n**Insights**:\n\n- Datetime should not be used as a feature because of hight cardinality. We can either create Fourier-based features by checking seasonality trends.\n- There are some pronounced pikes on the first and latest month of the year, this is because holidays.\n- Clear seasonality trend over the cancellations.\n- Year 2017 is really noisy because we don't have a lot of data compared to 2018.","metadata":{}},{"cell_type":"code","source":"temp = total_df.rename(columns={\n    'arrival_year': 'year',\n    'arrival_month': 'month',\n    'arrival_date': 'day'\n})\n\ntotal_df['date'] = pd.to_datetime(temp[['year', 'month', 'day']], errors='coerce')\nto_plot = total_df.groupby(['date', 'set']).size().rename('booking_count').reset_index()\n\nfig, ax = plt.subplots(2, 1, figsize=(16, 12))\nsns.lineplot(data=to_plot, x='date', y='booking_count', hue='set',\n            hue_order=labels, ax=ax[0])\n\nto_plot = total_df.groupby(['date', 'set'])['booking_status'].mean().reset_index()\nsns.lineplot(data=to_plot, x='date', y='booking_status', hue='set',\n            hue_order=labels, ax=ax[1])\n\n\nax[0].set_title('Count of Bookings')\nax[1].set_title('Mean Booking Cancellations')\nplt.tight_layout()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-14T03:28:28.404545Z","iopub.execute_input":"2023-02-14T03:28:28.405096Z","iopub.status.idle":"2023-02-14T03:28:30.027821Z","shell.execute_reply.started":"2023-02-14T03:28:28.405046Z","shell.execute_reply":"2023-02-14T03:28:30.026417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wrong dates\n\nThe following code provides all dates that are wrong if we consider a traditional calendar.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\nwrong_dates = total_df[\n    ['arrival_year', 'arrival_month', 'arrival_date', 'set']\n].loc[total_df.date.isnull()]\n\ndisplay(\n    wrong_dates.groupby('set').apply(\n        lambda df: df[\n            ['arrival_year', 'arrival_month', 'arrival_date']\n        ].apply(tuple, axis=1).unique()\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:30.029258Z","iopub.execute_input":"2023-02-14T03:28:30.029604Z","iopub.status.idle":"2023-02-14T03:28:30.060181Z","shell.execute_reply.started":"2023-02-14T03:28:30.029574Z","shell.execute_reply":"2023-02-14T03:28:30.058431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Date-based Features","metadata":{}},{"cell_type":"code","source":"feature_dates = [\n    'year', 'month', 'week', 'day', 'dayofweek', 'quarter', 'dayofyear'\n]\nline_style=''\ntotal_df = process_time(total_df)\n\ncolumns = feature_dates\nn_cols = 3\nn_rows = math.ceil(len(columns)/n_cols)\nfig, ax = plt.subplots(n_rows, n_cols, figsize=(16, n_rows*5))\nax = ax.flatten()\n\nfor i, column in enumerate(columns):\n    plot_axes = [ax[i]]\n    plot_ordinals(column, ax[i])\n    ax[i].legend(fontsize=9)\n\n    # titles\n    ax[i].set_title(f'{column} Distribution');\n    ax[i].set_xlabel(None)\n    \nfor i in range(i+1, len(ax)):\n    ax[i].axis('off')","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:30.061959Z","iopub.execute_input":"2023-02-14T03:28:30.062335Z","iopub.status.idle":"2023-02-14T03:28:41.730613Z","shell.execute_reply.started":"2023-02-14T03:28:30.062302Z","shell.execute_reply":"2023-02-14T03:28:41.729664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Values\n\n**Insights**\n- There are no null values, this competition won't involve any null filling technique. We can use directly any model out of the box.","metadata":{}},{"cell_type":"code","source":"train_null = train_df.isnull().sum().rename('train')\ntest_null = test_df.isnull().sum().rename('test')\noriginal_train_null = original_train_df.isnull().sum().rename('original train')\noriginal_test_null = original_test_df.isnull().sum().rename('original test')\n\npd.concat([train_null, test_null, original_train_null, original_test_null], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:41.732001Z","iopub.execute_input":"2023-02-14T03:28:41.732564Z","iopub.status.idle":"2023-02-14T03:28:41.760716Z","shell.execute_reply.started":"2023-02-14T03:28:41.732529Z","shell.execute_reply":"2023-02-14T03:28:41.759487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Duplicates\n\n**Insights**:\n\n- There are lot of duplicates on test set. By Probbing the leaderboard one can assign the \"right\" value to this records.\n- Orginal train and original test also have lots of duplicates, if augmenting the dataset the value can lead to noisy predictions.","metadata":{}},{"cell_type":"code","source":"train_dups = train_df.duplicated().sum()\ntest_dups = test_df.duplicated().sum()\noriginal_train_dups = original_train_df.duplicated().sum()\noriginal_test_dups = original_test_df.duplicated().sum()\n\nprint(f'''-------------------------\ntrain:           {train_dups}\ntest:            {test_dups}\noriginal_train:  {original_train_dups}\noriginal_test:   {original_test_dups}\n''')","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:41.762032Z","iopub.execute_input":"2023-02-14T03:28:41.762358Z","iopub.status.idle":"2023-02-14T03:28:41.80948Z","shell.execute_reply.started":"2023-02-14T03:28:41.762328Z","shell.execute_reply":"2023-02-14T03:28:41.808212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Correlations","metadata":{"execution":{"iopub.status.busy":"2023-02-14T01:30:09.714406Z","iopub.execute_input":"2023-02-14T01:30:09.714849Z","iopub.status.idle":"2023-02-14T01:30:09.719988Z","shell.execute_reply.started":"2023-02-14T01:30:09.714809Z","shell.execute_reply":"2023-02-14T01:30:09.719061Z"}}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 3, figsize=(20, 20))\nfloat_types = [np.int64, np.float16, np.float32, np.float64]\nfloat_columns = train_df.select_dtypes(include=float_types).columns\ncbar_ax = fig.add_axes([.91, .39, .01, .2])\n\nnames = ['Train', 'Original']\nfor i, df in enumerate([train_df, original_train_df]):\n    \n    corr = df[float_columns].corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    sns.heatmap(\n        corr, mask=mask, cmap='inferno',\n        vmax=0.8, vmin=-1,\n        center=0, annot=False, fmt='.3f',\n        square=True, linewidths=.5,\n        ax=ax[i],\n        cbar=False,\n        cbar_ax=None\n    );\n\n    ax[i].set_title(f'Correlation matrix for {names[i]} df', fontsize=14)\n\ndf = test_df\nfloat_columns = test_df.select_dtypes(include=float_types).columns\ncorr = test_df[float_columns].corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nsns.heatmap(\n    corr, mask=mask, cmap='inferno',\n    vmax=0.8, vmin=-1,\n    center=0, annot=False, fmt='.3f',\n    square=True, linewidths=.5,\n    cbar_kws={\"shrink\":.5, 'orientation':'vertical'},\n    ax=ax[2],\n    cbar=True,\n    cbar_ax=cbar_ax\n);\nax[2].set_title(f'Correlation matrix for Test', fontsize=14)\nfig.tight_layout(rect=[0, 0, .9, 1]);","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-14T03:28:41.811046Z","iopub.execute_input":"2023-02-14T03:28:41.811541Z","iopub.status.idle":"2023-02-14T03:28:44.045665Z","shell.execute_reply.started":"2023-02-14T03:28:41.811503Z","shell.execute_reply":"2023-02-14T03:28:44.044441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Baseline","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\ncv = StratifiedKFold(5, shuffle=True, random_state=42)\nfeatures = train_df.columns.difference(['booking_status'])\n\nX = train_df[features]\nX_ts = test_df[features]\ny = train_df.booking_status\n\naucs = []\ntest_preds = []\nmodels = []\noof_preds = pd.Series(0, index=train_df.index)\nstart = time()\n\nfor fold, (tr_ix, vl_ix) in enumerate(cv.split(train_df, train_df.booking_status)):\n    start_fold = time()\n    X_tr, y_tr = X.loc[tr_ix].copy(), y.loc[tr_ix]\n    X_vl, y_vl = X.loc[vl_ix].copy(), y.loc[vl_ix]\n    X_ts = test_df[features].copy()\n    \n    # concat orginal df\n    X_tr = pd.concat([X_tr, original_train_df[features]], ignore_index=True)\n    y_tr = pd.concat([y_tr, original_train_df.booking_status], ignore_index=True)\n    \n    # feature engineering\n    X_tr = process_time(X_tr)\n    X_vl = process_time(X_vl)\n    X_ts = process_time(X_ts)\n    \n    model = LGBMClassifier(max_depth=3, random_state=42)\n    model.fit(X_tr, y_tr)\n    y_pred = model.predict_proba(X_vl)[:, 1]\n    oof_preds.iloc[vl_ix] = y_pred\n    \n    test_preds.append(model.predict_proba(X_ts)[:, 1])\n    aucs.append(roc_auc_score(y_vl, y_pred))\n    models.append(model)\n    \n    print('_' * 30)\n    print(f'Fold: {fold} - {timedelta(seconds=int(time()-start))}')\n    print(f'Fold roc AUC  : ', aucs[-1])\n    print(f'Train Time taken :  {timedelta(seconds=int(time()-start_fold))}')\n    print()\n    \nprint(f'Mean ROC AUC:  {Fore.GREEN}{np.mean(aucs)}{Style.RESET_ALL}')","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:35:44.217318Z","iopub.execute_input":"2023-02-14T03:35:44.217736Z","iopub.status.idle":"2023-02-14T03:35:49.247515Z","shell.execute_reply.started":"2023-02-14T03:35:44.217704Z","shell.execute_reply":"2023-02-14T03:35:49.246347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/playground-series-s3e7/sample_submission.csv', index_col=0)\nsubmission.booking_status = np.mean(test_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:47.05959Z","iopub.execute_input":"2023-02-14T03:28:47.064368Z","iopub.status.idle":"2023-02-14T03:28:47.083366Z","shell.execute_reply.started":"2023-02-14T03:28:47.064307Z","shell.execute_reply":"2023-02-14T03:28:47.082011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\nsns.histplot(oof_preds, alpha=0.3, color=palette[0], ax=ax, bins=50, stat='percent', label='oof')\nsns.histplot(submission.booking_status, ax=ax, alpha=0.3, color=palette[1], bins=50, stat='percent', label='test')\n\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-02-14T03:28:47.084837Z","iopub.execute_input":"2023-02-14T03:28:47.085304Z","iopub.status.idle":"2023-02-14T03:28:47.699126Z","shell.execute_reply.started":"2023-02-14T03:28:47.085269Z","shell.execute_reply":"2023-02-14T03:28:47.697733Z"},"trusted":true},"execution_count":null,"outputs":[]}]}